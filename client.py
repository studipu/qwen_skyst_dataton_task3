"""
FastMCP Blockchain Analysis - Qwen Client
Qwen2.5-1.5B ëª¨ë¸ê³¼ MCP ì„œë²„ë¥¼ ì—°ê²°í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸
"""

import asyncio
import json
import re
import torch
import subprocess
import time
from typing import Dict, List, Any, Optional
from datetime import datetime

# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from huggingface_hub import login

# ë‚´ë¶€ ëª¨ë“ˆ
from config import CONFIG
from utils import print_debug, parse_mcp_call, save_demo_result, timing_decorator

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

class QwenModelManager:
    """Qwen2.5-1.5B ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self, model_name: str = None, cache_dir: str = None):
        self.model_name = model_name or CONFIG.MODEL_NAME
        self.cache_dir = cache_dir or CONFIG.CACHE_DIR
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.model = None
        self.tokenizer = None
        
        print_debug(f"Initializing Qwen Model Manager", {
            'model': self.model_name,
            'device': self.device,
            'cache_dir': self.cache_dir
        })
    
    async def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°)"""
        print("ğŸ¤– Loading Qwen2.5-1.5B model...")
        
        try:
            # HuggingFace í† í° ì„¤ì •
            if CONFIG.HUGGINGFACE_TOKEN:
                login(token=CONFIG.HUGGINGFACE_TOKEN)
                print_debug("HuggingFace token configured")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ“ Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir,
                trust_remote_code=True,
                token=CONFIG.HUGGINGFACE_TOKEN if CONFIG.HUGGINGFACE_TOKEN else None
            )
            
            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            print("ğŸ§  Loading model...")
            model_kwargs = {
                "cache_dir": self.cache_dir,
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
            }
            
            if CONFIG.HUGGINGFACE_TOKEN:
                model_kwargs["token"] = CONFIG.HUGGINGFACE_TOKEN
            
            # GPU ì„¤ì •
            if self.device == "cuda":
                model_kwargs["device_map"] = "auto"
                model_kwargs["low_cpu_mem_usage"] = True
                print("ğŸš€ Using GPU acceleration")
            else:
                print("ğŸŒ Using CPU (consider GPU for better performance)")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # CPUë¡œ ëª…ì‹œì  ì´ë™ (í•„ìš”í•œ ê²½ìš°)
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… Qwen2.5-1.5B model loaded successfully!")
            print(f"ğŸ“Š Model size: ~1.5B parameters")
            print(f"ğŸ’¾ Memory usage: ~{self._estimate_memory_usage():.1f}GB")
            
        except Exception as e:
            print(f"âŒ Error loading Qwen model: {e}")
            raise
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if self.model is None:
            return 0.0
        
        param_count = sum(p.numel() for p in self.model.parameters())
        bytes_per_param = 2 if self.device == "cuda" else 4  # fp16 vs fp32
        return (param_count * bytes_per_param) / (1024**3)  # GB
    
    def build_system_prompt(self, available_tools: List[Dict]) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        tools_desc = "\n".join([
            f"- {tool['name']}: {tool.get('description', 'No description available')}"
            for tool in available_tools
        ])
        
        return f"""You are an AI assistant specialized in blockchain and DeFi analysis. You have access to powerful tools via MCP (Model Context Protocol).

Available MCP Tools:
{tools_desc}

IMPORTANT: When you need to use a tool, respond with this EXACT format:
<mcp_call>
{{"tool": "tool_name", "arguments": {{"param": "value"}}}}
</mcp_call>

Guidelines:
1. Use tools when you need real-time blockchain data or DeFi information
2. Provide helpful analysis and insights based on the tool results
3. If you don't need tools, respond normally with your knowledge
4. Always be accurate with wallet addresses (42 characters starting with 0x)
5. Explain complex DeFi concepts in simple terms
6. Format large numbers in readable format (e.g., $1.5M instead of $1500000)

Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    @timing_decorator
    async def generate_response(self, prompt: str) -> str:
        """ì‘ë‹µ ìƒì„±"""
        try:
            if not self.model or not self.tokenizer:
                return "Error: Model not loaded"
            
            print_debug("Generating response", {"prompt_length": len(prompt)})
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096,  # 1.5B ëª¨ë¸ì—ì„œ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ìƒì„± ì„¤ì •
            generation_config = GenerationConfig(
                max_new_tokens=CONFIG.MAX_NEW_TOKENS,
                temperature=CONFIG.TEMPERATURE,
                top_p=CONFIG.TOP_P,
                top_k=CONFIG.TOP_K,
                do_sample=True,
                repetition_penalty=CONFIG.REPETITION_PENALTY,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3  # ë°˜ë³µ ë°©ì§€
            )
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config,
                    use_cache=True
                )
            
            # ë””ì½”ë”© (ì…ë ¥ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            print_debug("Response generated", {"response_length": len(generated_text)})
            
            return generated_text
            
        except Exception as e:
            print_debug(f"Generation error: {e}", level="ERROR")
            return f"Error generating response: {str(e)}"
    
    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print_debug("Memory cache cleared")

class MCPConnector:
    """MCP ì„œë²„ ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self, server_command: str = "python server.py"):
        self.server_command = server_command
        self.server_process = None
        self.available_tools = []
        self.is_connected = False
        
    async def start_mcp_server(self):
        """MCP ì„œë²„ ì‹œì‘"""
        try:
            print("ğŸ”§ Starting MCP server...")
            
            # ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
            self.server_process = subprocess.Popen(
                self.server_command.split(),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
            await asyncio.sleep(3)
            
            # ì„œë²„ ìƒíƒœ í™•ì¸
            if self.server_process.poll() is None:
                print("âœ… MCP server started successfully")
                self.is_connected = True
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì„¤ì •
                self.available_tools = [
                    {
                        'name': 'analyze_wallet_portfolio',
                        'description': 'Analyze wallet token portfolio with USD values'
                    },
                    {
                        'name': 'get_wallet_transaction_history', 
                        'description': 'Get recent transaction history for a wallet'
                    },
                    {
                        'name': 'get_defi_protocol_info',
                        'description': 'Get DeFi protocol TVL and information'
                    },
                    {
                        'name': 'get_defi_market_overview',
                        'description': 'Get overall DeFi market overview and statistics'
                    },
                    {
                        'name': 'analyze_wallet_defi_exposure',
                        'description': 'Analyze wallet DeFi protocol exposure and risk'
                    },
                    {
                        'name': 'compare_defi_protocols',
                        'description': 'Compare two DeFi protocols by TVL and features'
                    }
                ]
                
                print(f"ğŸ”§ Available tools: {len(self.available_tools)}")
                for tool in self.available_tools:
                    print(f"  â€¢ {tool['name']}")
                
            else:
                raise Exception("Server failed to start")
                
        except Exception as e:
            print(f"âŒ Failed to start MCP server: {e}")
            raise
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """MCP ë„êµ¬ í˜¸ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""
        if not self.is_connected:
            return {'error': 'MCP server not connected'}
        
        print_debug(f"Calling MCP tool: {tool_name}", arguments)
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MCP í”„ë¡œí† ì½œì„ í†µí•´ ì„œë²„ì™€ í†µì‹ 
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ì§ì ‘ import
        try:
            from server import (
                analyze_wallet_portfolio,
                get_wallet_transaction_history,
                get_defi_protocol_info,
                get_defi_market_overview,
                analyze_wallet_defi_exposure,
                compare_defi_protocols
            )
            
            # í•¨ìˆ˜ ë§¤í•‘
            tool_functions = {
                'analyze_wallet_portfolio': analyze_wallet_portfolio,
                'get_wallet_transaction_history': get_wallet_transaction_history,
                'get_defi_protocol_info': get_defi_protocol_info,
                'get_defi_market_overview': get_defi_market_overview,
                'analyze_wallet_defi_exposure': analyze_wallet_defi_exposure,
                'compare_defi_protocols': compare_defi_protocols
            }
            
            if tool_name in tool_functions:
                result = await tool_functions[tool_name](**arguments)
                print_debug("Tool execution successful", {"tool": tool_name})
                return result
            else:
                return {'error': f'Unknown tool: {tool_name}'}
                
        except Exception as e:
            print_debug(f"Tool execution failed: {e}", level="ERROR")
            return {'error': f'Tool execution failed: {str(e)}'}
    
    def stop_server(self):
        """MCP ì„œë²„ ì¤‘ì§€"""
        if self.server_process:
            self.server_process.terminate()
            self.server_process.wait()
            print("ğŸ›‘ MCP server stopped")

class QwenMcpClient:
    """Qwen MCP í†µí•© í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.model_manager = QwenModelManager()
        self.mcp_connector = MCPConnector()
        self.conversation_history = []
        
        print("ğŸš€ Qwen MCP Client initialized")
    
    async def initialize(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ Initializing Qwen MCP Client...")
        
        # ëª¨ë¸ ë¡œë“œì™€ MCP ì„œë²„ ì‹œì‘ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        await asyncio.gather(
            self.model_manager.load_model(),
            self.mcp_connector.start_mcp_server()
        )
        
        print("âœ… Qwen MCP Client ready!")
    
    def _build_conversation_prompt(self) -> str:
        """ëŒ€í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        system_prompt = self.model_manager.build_system_prompt(self.mcp_connector.available_tools)
        prompt = system_prompt + "\n\nConversation:\n"
        
        for msg in self.conversation_history:
            role = msg["role"]
            content = msg["content"]
            
            if role == "user":
                prompt += f"Human: {content}\n"
            elif role == "assistant":
                prompt += f"Assistant: {content}\n"
            elif role == "tool_result":
                prompt += f"Tool Result: {content}\n"
        
        prompt += "Assistant: "
        return prompt
    
    async def chat(self, user_message: str, max_iterations: int = 3) -> str:
        """ì‚¬ìš©ìì™€ ì±„íŒ…"""
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        self.conversation_history.append({"role": "user", "content": user_message})
        print_debug(f"User message: {user_message}")
        
        iteration = 0
        
        while iteration < max_iterations:
            # í˜„ì¬ ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_conversation_prompt()
            
            # Qwen ëª¨ë¸ì—ì„œ ì‘ë‹µ ìƒì„±
            response = await self.model_manager.generate_response(prompt)
            
            print_debug(f"Qwen response (iteration {iteration + 1})", {
                "response_preview": response[:100] + "..." if len(response) > 100 else response
            })
            
            # MCP ë„êµ¬ í˜¸ì¶œ íŒŒì‹±
            tool_call = parse_mcp_call(response)
            
            if tool_call and 'tool' in tool_call:
                # MCP ë„êµ¬ ì‹¤í–‰
                tool_name = tool_call['tool']
                arguments = tool_call.get('arguments', {})
                
                print(f"ğŸ”§ Executing tool: {tool_name}")
                tool_result = await self.mcp_connector.call_tool(tool_name, arguments)
                
                # ë„êµ¬ ê²°ê³¼ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                self.conversation_history.append({
                    "role": "assistant", 
                    "content": f"I'll use the {tool_name} tool to get this information."
                })
                self.conversation_history.append({
                    "role": "tool_result", 
                    "content": json.dumps(tool_result, indent=2)
                })
                
                iteration += 1
            else:
                # ìµœì¢… ì‘ë‹µ
                self.conversation_history.append({"role": "assistant", "content": response})
                return response
        
        return "I've reached the maximum number of tool calls. Please try a simpler query."
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print_debug("Conversation history cleared")
    
    def get_statistics(self) -> Dict[str, Any]:
        """í´ë¼ì´ì–¸íŠ¸ í†µê³„ ì •ë³´"""
        return {
            "conversation_messages": len(self.conversation_history),
            "tool_calls_made": len([msg for msg in self.conversation_history if msg["role"] == "tool_result"]),
            "model_info": {
                "name": self.model_manager.model_name,
                "device": self.model_manager.device,
                "memory_usage_gb": self.model_manager._estimate_memory_usage()
            },
            "mcp_status": {
                "connected": self.mcp_connector.is_connected,
                "available_tools": len(self.mcp_connector.available_tools)
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("ğŸ§¹ Cleaning up resources...")
        
        # MCP ì„œë²„ ì¢…ë£Œ
        self.mcp_connector.stop_server()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self.model_manager.clear_cache()
        
        print("âœ… Cleanup completed")

class ChatInterface:
    """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, client: QwenMcpClient):
        self.client = client
    
    async def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\n" + "="*70)
        print("ğŸ¤– Qwen2.5-1.5B Blockchain Analysis Chat")
        print("="*70)
        print("Commands:")
        print("  â€¢ quit/exit - ì¢…ë£Œ")
        print("  â€¢ clear - ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
        print("  â€¢ stats - ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
        print("  â€¢ demo - ìë™ ë°ëª¨ ì‹¤í–‰")
        print("\nğŸ’¡ Example queries:")
        print("  â€¢ 'Analyze wallet 0x742d...' - ì§€ê°‘ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„")
        print("  â€¢ 'What is the TVL of Uniswap?' - DeFi í”„ë¡œí† ì½œ ì •ë³´")
        print("  â€¢ 'Show me DeFi market overview' - ì „ì²´ ì‹œì¥ í˜„í™©")
        print("  â€¢ 'Compare Uniswap and SushiSwap' - í”„ë¡œí† ì½œ ë¹„êµ")
        print("="*70)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ Goodbye!")
                    break
                
                elif user_input.lower() == 'clear':
                    self.client.clear_conversation()
                    print("ğŸ§¹ Conversation cleared")
                    continue
                
                elif user_input.lower() == 'stats':
                    stats = self.client.get_statistics()
                    print("ğŸ“Š System Statistics:")
                    print(json.dumps(stats, indent=2))
                    continue
                
                elif user_input.lower() == 'demo':
                    await self.run_demo()
                    continue
                
                elif not user_input:
                    continue
                
                # ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
                print("ğŸ¤” Processing...")
                start_time = time.time()
                
                response = await self.client.chat(user_input)
                
                end_time = time.time()
                print(f"\nğŸ¤– Assistant: {response}")
                
                if CONFIG.DEBUG_MODE:
                    print(f"\nâ±ï¸ Response time: {end_time - start_time:.2f}s")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Interrupted. Goodbye!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")
                if CONFIG.DEBUG_MODE:
                    import traceback
                    traceback.print_exc()
    
    async def run_demo(self):
        """ìë™ ë°ëª¨ ì‹¤í–‰"""
        print("\nğŸ­ Running automatic demo...")
        
        for i, query in enumerate(CONFIG.DEMO_QUERIES, 1):
            print(f"\n{'='*50}")
            print(f"ğŸ¯ Demo {i}/{len(CONFIG.DEMO_QUERIES)}: {query}")
            print('='*50)
            
            try:
                start_time = time.time()
                response = await self.client.chat(query)
                end_time = time.time()
                
                print(f"ğŸ“ Response: {response}")
                print(f"â±ï¸ Time: {end_time - start_time:.2f}s")
                
                # ë°ëª¨ ê²°ê³¼ ì €ì¥
                save_demo_result(query, {
                    'response': response,
                    'time_taken': end_time - start_time,
                    'success': True
                })
                
                # ë°ëª¨ ê°„ ì ì‹œ ëŒ€ê¸°
                if i < len(CONFIG.DEMO_QUERIES):
                    print("\nâ³ Waiting 3 seconds...")
                    await asyncio.sleep(3)
                
            except Exception as e:
                print(f"âŒ Demo {i} failed: {e}")
                save_demo_result(query, {
                    'error': str(e),
                    'success': False
                })
        
        print(f"\nâœ… Demo completed! Results saved to examples/demo_results.json")
    
    async def batch_mode(self, queries: List[str]):
        """ë°°ì¹˜ ëª¨ë“œ - ì—¬ëŸ¬ ì§ˆë¬¸ì„ ìˆœì°¨ ì²˜ë¦¬"""
        print(f"\nğŸ“‹ Batch processing {len(queries)} queries...")
        
        results = []
        
        for i, query in enumerate(queries, 1):
            print(f"\nğŸ”„ Processing query {i}/{len(queries)}: {query}")
            
            try:
                start_time = time.time()
                response = await self.client.chat(query)
                end_time = time.time()
                
                result = {
                    "query": query,
                    "response": response,
                    "time_taken": end_time - start_time,
                    "success": True
                }
                
                results.append(result)
                print(f"âœ… Query {i} completed in {end_time - start_time:.2f}s")
                
            except Exception as e:
                result = {
                    "query": query,
                    "error": str(e),
                    "success": False
                }
                results.append(result)
                print(f"âŒ Query {i} failed: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.client.model_manager.clear_cache()
        
        return results

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    try:
        print("""
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
    â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â• 
    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     
     â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•šâ•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•    â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•     
    
    Qwen2.5-1.5B + FastMCP Blockchain Analysis System
        """)
        
        # ì„¤ì • ê²€ì¦
        if not CONFIG.validate():
            print("âŒ Configuration validation failed")
            return
        
        # Qwen MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = QwenMcpClient()
        
        print("ğŸ”§ Initializing system...")
        await client.initialize()
        
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        chat_interface = ChatInterface(client)
        
        # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
        if len(sys.argv) > 1:
            mode = sys.argv[1].lower()
            
            if mode in ['interactive', 'i']:
                await chat_interface.interactive_mode()
            
            elif mode == 'demo':
                await chat_interface.run_demo()
            
            elif mode == 'batch' and len(sys.argv) > 2:
                # ë°°ì¹˜ ëª¨ë“œ - ëª…ë ¹í–‰ì—ì„œ ì§ˆë¬¸ë“¤ ì…ë ¥
                queries = sys.argv[2:]
                results = await chat_interface.batch_mode(queries)
                
                print("\nğŸ“Š Batch Results Summary:")
                successful = sum(1 for r in results if r.get('success', False))
                print(f"âœ… Successful: {successful}/{len(results)}")
                
                if CONFIG.DEBUG_MODE:
                    for result in results:
                        status = "âœ…" if result.get('success', False) else "âŒ" 
                        print(f"{status} {result['query']}")
            
            elif mode == 'quick' and len(sys.argv) > 2:
                # ë¹ ë¥¸ ì§ˆë¬¸ ëª¨ë“œ
                query = " ".join(sys.argv[2:])
                print(f"â“ Quick query: {query}")
                
                response = await client.chat(query)
                print(f"ğŸ¤– Response: {response}")
            
            else:
                print("Usage: python client.py [interactive|demo|batch|quick] [queries...]")
                print("\nExamples:")
                print("  python client.py interactive")
                print("  python client.py demo")
                print('  python client.py quick "What is Uniswap TVL?"')
                print('  python client.py batch "Show DeFi overview" "Analyze wallet 0x123..."')
        
        else:
            # ê¸°ë³¸ê°’: ëŒ€í™”í˜• ëª¨ë“œ
            await chat_interface.interactive_mode()
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ System interrupted")
    except Exception as e:
        print(f"âŒ System error: {e}")
        if CONFIG.DEBUG_MODE:
            import traceback
            traceback.print_exc()
    finally:
        if 'client' in locals():
            await client.cleanup()

# ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
async def test_qwen_model():
    """Qwen ëª¨ë¸ ê°œë³„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing Qwen model independently...")
    
    model_manager = QwenModelManager()
    await model_manager.load_model()
    
    test_prompt = """You are a blockchain analyst. Answer this question: What is DeFi?

Answer: """
    
    response = await model_manager.generate_response(test_prompt)
    print(f"âœ… Model test response: {response}")
    
    model_manager.clear_cache()

async def test_mcp_connection():
    """MCP ì—°ê²° ê°œë³„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing MCP connection independently...")
    
    mcp_connector = MCPConnector()
    await mcp_connector.start_mcp_server()
    
    # í…ŒìŠ¤íŠ¸ ë„êµ¬ í˜¸ì¶œ
    test_result = await mcp_connector.call_tool('get_defi_market_overview', {})
    print(f"âœ… MCP test result: {test_result}")
    
    mcp_connector.stop_server()

if __name__ == "__main__":
    import sys
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        if len(sys.argv) > 2:
            if sys.argv[2] == 'model':
                asyncio.run(test_qwen_model())
            elif sys.argv[2] == 'mcp':
                asyncio.run(test_mcp_connection())
            else:
                print("Usage: python client.py test [model|mcp]")
        else:
            print("Running all tests...")
            asyncio.run(test_qwen_model())
            asyncio.run(test_mcp_connection())
    else:
        # ì¼ë°˜ ì‹¤í–‰
        asyncio.run(main())